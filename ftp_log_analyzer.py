import boto3
import argparse
import json
import os
import sys
import random
import logging
from datetime import datetime
from typing import List, Optional, Set
from botocore.exceptions import ClientError

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

def lire_et_filtrer_logs(
    chemin_fichier: str, 
    max_chars: int = 150000, 
    keywords: Optional[List[str]] = None,
    context_lines: int = 3
) -> str:
    """
    Lit le fichier de logs et filtre les entr√©es int√©ressantes.
    Optimis√© pour les gros fichiers : parcourt tout le fichier et garde le contexte.
    """
    if not os.path.exists(chemin_fichier):
        logger.error(f"Le fichier '{chemin_fichier}' n'existe pas.")
        sys.exit(1)

    suspicious_keywords = keywords or [
        'FAIL', 'error', 'denied', 'refused', 'incorrect', 
        '530', '550', '421', 'root', 'admin'
    ]
    
    important_lines = []
    buffer = [] # Pour garder le contexte pr√©c√©dent
    total_lines = 0
    selected_count = 0
    
    logger.info(f"Analyse du fichier : {chemin_fichier}...")
    
    try:
        with open(chemin_fichier, 'r', encoding='utf-8', errors='replace') as f:
            for line in f:
                total_lines += 1
                line_lower = line.lower()
                
                # On garde un buffer circulaire pour le contexte
                buffer.append(line)
                if len(buffer) > context_lines + 1:
                    buffer.pop(0)
                
                is_suspicious = any(k.lower() in line_lower for k in suspicious_keywords)
                is_connection = 'connect' in line_lower or 'ok login' in line_lower
                
                if is_suspicious or is_connection:
                    # Ajouter le contexte si ce n'est pas d√©j√† fait
                    if is_suspicious and len(buffer) > 1:
                        important_lines.append(f"--- CONTEXTE ---\n" + "".join(buffer[:-1]))
                        # On vide le buffer pour ne pas le rajouter plusieurs fois
                        buffer = [line]
                    
                    important_lines.append(line)
                    selected_count += 1
                elif random.random() < 0.005: # 0.5% d'√©chantillon pour le volume global
                    important_lines.append(f"[ECHANTILLON] {line}")
                    selected_count += 1

                # V√©rification de la taille p√©riodiquement
                if len("".join(important_lines)) > max_chars * 1.2:
                    # Si on d√©passe vraiment trop, on fait une purge intelligente
                    # (On garde les 1000 premi√®res et les 1000 derni√®res lignes par exemple)
                    logger.warning("Limite de taille approch√©e, √©chantillonnage plus agressif activ√©.")
                    if len(important_lines) > 2000:
                        important_lines = important_lines[:1000] + ["\n... [TRONQU√â AU MILIEU] ...\n"] + important_lines[-1000:]
        
        final_content = "".join(important_lines)
        if len(final_content) > max_chars:
            logger.info(f"Tronquage final √† {max_chars} caract√®res.")
            final_content = final_content[:max_chars] + "\n... [FIN TRONQU√âE] ..."

        logger.info(f"Analyse termin√©e : {total_lines} lignes lues. {selected_count} segments s√©lectionn√©s.")
        return final_content

    except Exception as e:
        logger.error(f"Erreur lors de la lecture : {e}")
        sys.exit(1)

def analyser_avec_bedrock(
    contenu_log: str, 
    model_id: str = "anthropic.claude-3-sonnet-20240229-v1:0", 
    region: str = "us-west-2"
) -> str:
    """Envoie les logs √† Amazon Bedrock pour analyse cybers√©curit√©."""
    logger.info(f"Initialisation AWS (R√©gion: {region}, Mod√®le: {model_id})...")
    
    try:
        bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name=region)
    except Exception as e:
        logger.error(f"√âchec initialisation AWS : {e}")
        sys.exit(1)

    system_prompt = (
        "Tu es un expert en cybers√©curit√© senior (SOC Analyst). "
        "Ton r√¥le est d'analyser des logs FTP pr√©-filtr√©s pour d√©tecter des anomalies, "
        "des tentatives de bruteforce, ou des exfiltrations de donn√©es."
    )

    user_message = (
        f"Voici un extrait de logs FTP. Note: certains passages sont √©chantillonn√©s ou contiennent du contexte.\n\n"
        f"<logs>\n{contenu_log}\n</logs>\n\n"
        "Produis un rapport de s√©curit√© en Markdown structur√© comme suit :\n"
        "1. **R√©sum√© Ex√©cutif** (Niveau de risque global)\n"
        "2. **Indicateurs de Compromission (IoCs)** : IPs suspectes, comptes vis√©s.\n"
        "3. **Chronologie des √âv√©nements** : Analyse des s√©quences suspectes.\n"
        "4. **Actions Correctives Imm√©diates** (Blocage IP, changement de mot de passe, etc.)."
    )

    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4096,
        "temperature": 0.0,
        "messages": [{"role": "user", "content": user_message}],
        "system": system_prompt
    }

    try:
        logger.info("Analyse en cours par l'IA...")
        response = bedrock_runtime.invoke_model(
            body=json.dumps(payload),
            modelId=model_id,
            accept='application/json',
            contentType='application/json'
        )
        response_body = json.loads(response.get('body').read())
        return response_body.get('content')[0].get('text')
    except ClientError as e:
        logger.error(f"Erreur API Bedrock : {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Erreur inattendue lors de l'appel IA : {e}")
        sys.exit(1)

def sauvegarder_rapport(contenu: str, chemin: str) -> None:
    try:
        with open(chemin, 'w', encoding='utf-8') as f:
            f.write(contenu)
        logger.info(f"Rapport g√©n√©r√© avec succ√®s : {chemin}")
    except Exception as e:
        logger.error(f"Erreur lors de l'√©criture du rapport : {e}")

def main():
    parser = argparse.ArgumentParser(
        description="üöÄ Analyseur de Logs FTP intelligent via Amazon Bedrock.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("logfile", help="Chemin vers le fichier de logs")
    parser.add_argument("--output", "-o", default="rapport_securite.md", help="Nom du fichier de sortie")
    parser.add_argument("--max-size", type=int, default=150000, help="Limite de caract√®res envoy√©s √† l'IA")
    parser.add_argument("--context", type=int, default=3, help="Nombre de lignes de contexte avant chaque alerte")
    parser.add_argument("--keywords", nargs='+', help="Mots-cl√©s personnalis√©s √† filtrer")
    parser.add_argument("--region", "-r", default="us-west-2", help="R√©gion AWS")
    parser.add_argument("--model", "-m", default="anthropic.claude-3-sonnet-20240229-v1:0", help="ID du mod√®le Bedrock")
    parser.add_argument("--verbose", "-v", action="store_true", help="Active le mode verbeux (DEBUG)")

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    # 1. Pipeline de traitement
    start_time = datetime.now()
    
    log_content = lire_et_filtrer_logs(
        args.logfile, 
        max_chars=args.max_size, 
        keywords=args.keywords,
        context_lines=args.context
    )

    if not log_content.strip():
        logger.warning("Aucune donn√©e pertinente identifi√©e. Fin du traitement.")
        sys.exit(0)

    # 2. IA
    rapport = analyser_avec_bedrock(log_content, model_id=args.model, region=args.region)

    # 3. Sortie
    sauvegarder_rapport(rapport, args.output)
    
    duration = datetime.now() - start_time
    logger.info(f"Temps total d'ex√©cution : {duration.total_seconds():.2f} secondes.")

if __name__ == "__main__":
    main()
